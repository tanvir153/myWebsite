{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995eda29",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 137) (1572214962.py, line 137)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 137\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 137)\n"
     ]
    }
   ],
   "source": [
    "# Importing all necessary libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('stopwords')  # Source: NLTK Documentation (2023)\n",
    "nltk.download('punkt')  # Source: NLTK Documentation (2023)\n",
    "\n",
    "# Function to load and parse JSON data into a DataFrame\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSON file and converts it into a Pandas DataFrame.\n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the JSON data.\n",
    "    \"\"\"\n",
    "    print(\"Loading dataset...\")  # Source: pandas read_json documentation\n",
    "    try:\n",
    "        data = pd.read_json(file_path, lines=True)\n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        return data\n",
    "    except ValueError as e:\n",
    "        print(f\"Error loading JSON file: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Function to create visualizations\n",
    "def create_visualizations(data, asin):\n",
    "    \"\"\"\n",
    "    Generate visualizations for a given product dataset.\n",
    "    Args:\n",
    "        data (DataFrame): Product-specific data.\n",
    "        asin (str): Product ID.\n",
    "    \"\"\"\n",
    "    # Bar Plot for Ratings Distribution\n",
    "    plt.figure(figsize=(10, 6))  # Citation: Matplotlib official documentation\n",
    "    sns.countplot(data=data, x='overall', palette='coolwarm')\n",
    "    plt.title(f\"Ratings Distribution for Product {asin}\", fontsize=14)\n",
    "    plt.xlabel(\"Ratings\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.savefig(f'ratings_distribution_{asin}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Word Cloud for Reviews\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')  # Source: NLTK tokenization documentation\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))  # Source: NLTK stopwords documentation\n",
    "\n",
    "    all_words = \" \".join(data['reviewText'])\n",
    "    tokens = [stemmer.stem(word) for word in tokenizer.tokenize(all_words.lower()) if word not in stop_words]\n",
    "    word_freq = Counter(tokens)\n",
    "\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='black', colormap='viridis').generate_from_frequencies(word_freq)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Word Cloud for Product {asin}\", fontsize=14)\n",
    "    plt.savefig(f'wordcloud_{asin}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Sentiment Distribution\n",
    "    plt.figure(figsize=(10, 6))  # Citation: Seaborn histplot official documentation\n",
    "    sns.histplot(data['sentiment'], kde=True, color='purple', bins=20)\n",
    "    plt.title(f\"Sentiment Analysis for Product {asin}\", fontsize=14)\n",
    "    plt.xlabel(\"Sentiment Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.savefig(f'sentiment_distribution_{asin}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Sentiment Trend Over Time\n",
    "    data['reviewTime'] = pd.to_datetime(data['reviewTime'])\n",
    "    monthly_sentiment = data.groupby(data['reviewTime'].dt.to_period(\"M\"))['sentiment'].mean()\n",
    "    monthly_sentiment.plot(kind='line', marker='o', figsize=(10, 6), color='orange')  # pandas plot function\n",
    "    plt.title(f\"Sentiment Trend Over Time for Product {asin}\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Average Sentiment Score\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'sentiment_trend_{asin}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Bubble Chart: Ratings vs Votes\n",
    "    print(f\"Creating Bubble Chart for {asin}...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(\n",
    "        x=data['vote'], \n",
    "        y=data['overall'], \n",
    "        size=data['vote'], \n",
    "        sizes=(20, 300), \n",
    "        hue=data['overall'], \n",
    "        palette='Spectral', \n",
    "        alpha=0.7\n",
    "    )\n",
    "    plt.title(f\"Bubble Chart: Ratings vs Votes for Product {asin}\", fontsize=14)\n",
    "    plt.xlabel(\"Votes\")\n",
    "    plt.ylabel(\"Ratings\")\n",
    "    plt.legend(title=\"Ratings\", loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'bubble_chart_{asin}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Interactive Dashboard with Plotly\n",
    "    print(\"Creating interactive dashboard...\")  # Source: Plotly Documentation\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Ratings vs Votes\", \"Sentiment Distribution\"))\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=data['vote'], y=data['overall'], mode='markers',\n",
    "                   marker=dict(size=10, color=data['vote'], colorscale='Viridis', showscale=True),\n",
    "                   name=\"Votes vs Ratings\"),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=data['sentiment'], nbinsx=20, marker_color='purple', name=\"Sentiment\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "    fig.update_layout(height=600, width=1000, title_text=f\"Dashboard for Product {asin}\")\n",
    "    fig.write_html(f'interactive_dashboard_{asin}.html')\n",
    "    fig.show()\n",
    "\n",
    "# File path to the dataset\n",
    "file_path = \"D:\\\\Portfolio 2.0\\\\my_data.json\"\n",
    "\n",
    "# Load dataset\n",
    "reviews_df = load_data(file_path)\n",
    "\n",
    "# Data Cleaning and Preprocessing\n",
    "print(\"Data cleaning and preprocessing started...\")\n",
    "columns_to_keep = ['asin', 'reviewText', 'overall', 'unixReviewTime', 'summary', 'reviewerName', 'vote']\n",
    "reviews_df = reviews_df[columns_to_keep]\n",
    "\n",
    "# Handle missing values and clean the 'vote' column\n",
    "reviews_df['reviewerName'] = reviews_df['reviewerName'].fillna(\"Anonymous\")\n",
    "reviews_df['vote'] = reviews_df['vote'].fillna(0).astype(str).str.replace(',', '').astype(int)\n",
    "reviews_df.dropna(subset=['reviewText', 'overall'], inplace=True)\n",
    "\n",
    "# Convert UNIX timestamp to readable date\n",
    "reviews_df['reviewTime'] = pd.to_datetime(reviews_df['unixReviewTime'], unit='s')\n",
    "print(\"Data cleaning completed!\")\n",
    "\n",
    "# Analyze and Visualize Data\n",
    "products = reviews_df['asin'].unique()[3:6]  # Selecting a unique range of three products for analysis\n",
    "product_data = {asin: reviews_df[reviews_df['asin'] == asin] for asin in products}\n",
    "\n",
    "# Sentiment Analysis\n",
    "analyzer = SentimentIntensityAnalyzer()  # Source: VaderSentiment Documentation\n",
    "\n",
    "for asin, data in product_data.items():\n",
    "    print(f\"\\nAnalyzing Product: {asin}\")\n",
    "\n",
    "    # Sentiment Scores\n",
    "    data['sentiment'] = data['reviewText'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "\n",
    "    # Generate Visualizations\n",
    "    create_visualizations(data, asin)\n",
    "\n",
    "print(\"Analysis completed! Results saved.\")\n",
    "\n",
    "# Save Cleaned Data\n",
    "output_file = 'J119811_updated_with_dashboard.csv'  #  pandas to_csv documentation\n",
    "reviews_df.to_csv(output_file, index=False)\n",
    "print(f\"Cleaned dataset saved to {output_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
